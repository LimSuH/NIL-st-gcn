{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from natsort import natsorted\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pose_hrnet import get_pose_net\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from utils import pose_process, plot_pose\n",
    "\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mirror = np.concatenate([\n",
    "                [1,3,2,5,4,7,6,9,8,11,10,13,12,15,14,17,16],\n",
    "                [21,22,23,18,19,20],\n",
    "                np.arange(40,23,-1), np.arange(50,40,-1),\n",
    "                np.arange(51,55), np.arange(59,54,-1),\n",
    "                [69,68,67,66,71,70], [63,62,61,60,65,64],\n",
    "                np.arange(78,71,-1), np.arange(83,78,-1),\n",
    "                [88,87,86,85,84,91,90,89],\n",
    "                np.arange(113,134), np.arange(92,113)\n",
    "                ]) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_flip(img):\n",
    "    img_flip = cv2.flip(img, 1)\n",
    "    return np.stack([img, img_flip], axis=0) # [img, height, width, channel]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_numpy_totensor(img, mean, std):\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    for i in range(3):\n",
    "        # img: [batch, height, width, channel]?\n",
    "        img[:, :, :, i] = (img[:, :, :, i] - mean[i]) / std[i] \n",
    "    return torch.from_numpy(img).permute(0, 3, 1, 2) # [img, channel, height, width]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hm(hms_list):\n",
    "    assert isinstance(hms_list, list) # hms_list?\n",
    "    for hms in hms_list:\n",
    "        hms[1,:,:,:] = torch.flip(hms[1,index_mirror,:,:], [2]) # hms[1] double flipped, so original? why flip on [2] height?\n",
    "    \n",
    "    hm = torch.cat(hms_list, dim=0)\n",
    "    hm = torch.mean(hms, dim=0)\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'KETI_SL_0000002337.avi'\n",
    "video_path = os.path.join('./video', video_file)\n",
    "image_path = os.path.join('./image', video_file)\n",
    "\n",
    "mean = (0.485, 0.456, 0.406) # mean of normalized RGB? where did it come from?\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height = 1280, 720\n",
      "total 127 frames\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f'width, height = {w}, {h}')\n",
    "\n",
    "t = 0\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    t += 1\n",
    "\n",
    "    cv2.imwrite(os.path.join(image_path, f'frame_{t}.png'), image)\n",
    "\n",
    "cap.release()\n",
    "print(f'total {t} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height = 1280, 720\n",
      "total 127 frames\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f'width, height = {w}, {h}')\n",
    "\n",
    "margin = int((w - h) / 2)\n",
    "\n",
    "t = 0\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    t += 1\n",
    "\n",
    "    image = image[:, margin : margin + h]\n",
    "    image = cv2.resize(image, (512, 512))\n",
    "    # print(image.shape)\n",
    "    cv2.imwrite(os.path.join(image_path, f'frame_{t}.png'), image)\n",
    "\n",
    "cap.release()\n",
    "print(f'total {t} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "stack flipped: (2, 512, 512, 3)\n",
      "norm numpy tensored: torch.Size([2, 3, 512, 512])\n",
      "hms: torch.Size([2, 133, 128, 128])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "\n",
      "640\n",
      "stack flipped: (2, 640, 640, 3)\n",
      "norm numpy tensored: torch.Size([2, 3, 640, 640])\n",
      "hms: torch.Size([2, 133, 160, 160])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "\n",
      "merged: torch.Size([133, 128, 128])\n",
      "result: (133,)\n",
      "(133, 3)\n",
      "pred: [[2.2100000e+02 1.3100000e+02 7.3849005e-01]\n",
      " [2.4900000e+02 1.1100000e+02 8.0909246e-01]\n",
      " [2.0500000e+02 1.1100000e+02 8.3246070e-01]\n",
      " [2.9300000e+02 1.2500000e+02 8.2097149e-01]\n",
      " [1.8100000e+02 1.2100000e+02 7.5686377e-01]\n",
      " [3.4300000e+02 2.1700000e+02 5.2755237e-01]\n",
      " [1.5500000e+02 2.3500000e+02 5.4131627e-01]\n",
      " [3.1500000e+02 2.8500000e+02 4.7268251e-01]\n",
      " [4.5000000e+01 3.4300000e+02 6.0201252e-01]\n",
      " [2.3100000e+02 1.8500000e+02 6.3110894e-01]\n",
      " [8.1000000e+01 2.7500000e+02 6.8131435e-01]\n",
      " [2.3700000e+02 1.1500000e+02 2.0249465e-01]\n",
      " [2.1500000e+02 1.1500000e+02 1.6302747e-01]\n",
      " [2.1700000e+02 1.6100000e+02 8.4108949e-01]\n",
      " [2.2100000e+02 1.6100000e+02 8.4913999e-01]\n",
      " [2.2900000e+02 1.1100000e+02 1.4801044e+00]\n",
      " [2.2100000e+02 1.0900000e+02 1.4633440e+00]\n",
      " [2.4700000e+02 1.1100000e+02 1.7499853e+00]\n",
      " [2.4700000e+02 1.1100000e+02 1.7523301e+00]\n",
      " [2.3900000e+02 1.6100000e+02 1.5967500e+00]\n",
      " [2.3100000e+02 1.3900000e+02 1.9453623e+00]\n",
      " [2.3100000e+02 1.3900000e+02 1.9684006e+00]\n",
      " [2.3500000e+02 1.4100000e+02 1.4368649e+00]\n",
      " [1.8300000e+02 1.1300000e+02 8.1060231e-01]\n",
      " [1.8300000e+02 1.2500000e+02 7.4987322e-01]\n",
      " [1.8300000e+02 1.3500000e+02 8.3889842e-01]\n",
      " [1.8700000e+02 1.4700000e+02 7.9740310e-01]\n",
      " [1.8900000e+02 1.5900000e+02 8.0429065e-01]\n",
      " [1.9700000e+02 1.6900000e+02 7.6838350e-01]\n",
      " [2.0700000e+02 1.7900000e+02 7.5744033e-01]\n",
      " [2.1900000e+02 1.8300000e+02 7.6198041e-01]\n",
      " [2.3100000e+02 1.8300000e+02 7.0490217e-01]\n",
      " [2.4500000e+02 1.8100000e+02 7.3530751e-01]\n",
      " [2.5700000e+02 1.7900000e+02 7.6064092e-01]\n",
      " [2.6900000e+02 1.7300000e+02 7.8029525e-01]\n",
      " [2.7700000e+02 1.6300000e+02 7.6658905e-01]\n",
      " [2.8500000e+02 1.5300000e+02 7.9159033e-01]\n",
      " [2.8700000e+02 1.3900000e+02 8.2360131e-01]\n",
      " [2.9100000e+02 1.2700000e+02 7.3761213e-01]\n",
      " [2.9100000e+02 1.1700000e+02 7.8178048e-01]\n",
      " [1.9100000e+02 9.9000000e+01 9.0137529e-01]\n",
      " [1.9500000e+02 9.7000000e+01 8.4784943e-01]\n",
      " [2.0300000e+02 9.5000000e+01 8.2950026e-01]\n",
      " [2.0900000e+02 9.7000000e+01 8.8148940e-01]\n",
      " [2.1300000e+02 9.7000000e+01 7.6274264e-01]\n",
      " [2.3900000e+02 9.7000000e+01 7.4225056e-01]\n",
      " [2.4300000e+02 9.7000000e+01 8.3132672e-01]\n",
      " [2.4900000e+02 9.7000000e+01 7.5384110e-01]\n",
      " [2.6100000e+02 9.9000000e+01 7.7513385e-01]\n",
      " [2.6300000e+02 1.0100000e+02 8.2294649e-01]\n",
      " [2.2300000e+02 1.1100000e+02 8.4004849e-01]\n",
      " [2.2300000e+02 1.1700000e+02 8.7214267e-01]\n",
      " [2.2100000e+02 1.2500000e+02 8.7564147e-01]\n",
      " [2.2100000e+02 1.3300000e+02 8.0846560e-01]\n",
      " [2.1300000e+02 1.4100000e+02 9.1024899e-01]\n",
      " [2.1500000e+02 1.4100000e+02 8.4789306e-01]\n",
      " [2.2100000e+02 1.4300000e+02 7.6204419e-01]\n",
      " [2.2700000e+02 1.4300000e+02 8.0324030e-01]\n",
      " [2.2900000e+02 1.4300000e+02 8.4164071e-01]\n",
      " [1.9700000e+02 1.0900000e+02 8.0849862e-01]\n",
      " [2.0300000e+02 1.0900000e+02 7.8932911e-01]\n",
      " [2.0700000e+02 1.0900000e+02 8.2705957e-01]\n",
      " [2.1100000e+02 1.1100000e+02 8.1102133e-01]\n",
      " [2.0700000e+02 1.1100000e+02 8.9774859e-01]\n",
      " [2.0300000e+02 1.1100000e+02 8.9041990e-01]\n",
      " [2.4100000e+02 1.1100000e+02 7.1918917e-01]\n",
      " [2.4500000e+02 1.0900000e+02 7.8528416e-01]\n",
      " [2.5100000e+02 1.1100000e+02 7.8948778e-01]\n",
      " [2.5700000e+02 1.1100000e+02 8.1014848e-01]\n",
      " [2.5100000e+02 1.1100000e+02 8.8829678e-01]\n",
      " [2.4700000e+02 1.1100000e+02 8.7039673e-01]\n",
      " [2.0500000e+02 1.5500000e+02 9.0059018e-01]\n",
      " [2.0900000e+02 1.5300000e+02 8.1381941e-01]\n",
      " [2.1700000e+02 1.5300000e+02 8.4600174e-01]\n",
      " [2.1900000e+02 1.5300000e+02 8.6406678e-01]\n",
      " [2.2300000e+02 1.5300000e+02 8.2316387e-01]\n",
      " [2.3100000e+02 1.5500000e+02 7.6793420e-01]\n",
      " [2.3700000e+02 1.5700000e+02 8.4938276e-01]\n",
      " [2.3300000e+02 1.5900000e+02 7.9172075e-01]\n",
      " [2.2700000e+02 1.5900000e+02 7.9194641e-01]\n",
      " [2.1900000e+02 1.6100000e+02 8.1689167e-01]\n",
      " [2.1300000e+02 1.5900000e+02 8.5099900e-01]\n",
      " [2.0700000e+02 1.5900000e+02 8.5216570e-01]\n",
      " [2.0500000e+02 1.5500000e+02 8.9678383e-01]\n",
      " [2.1300000e+02 1.5500000e+02 8.5698152e-01]\n",
      " [2.1900000e+02 1.5500000e+02 8.9474028e-01]\n",
      " [2.2900000e+02 1.5500000e+02 7.9536229e-01]\n",
      " [2.3700000e+02 1.5700000e+02 8.5345018e-01]\n",
      " [2.2900000e+02 1.5900000e+02 7.8983408e-01]\n",
      " [2.1900000e+02 1.5900000e+02 8.4995276e-01]\n",
      " [2.1100000e+02 1.5900000e+02 8.5085428e-01]\n",
      " [2.3100000e+02 1.8100000e+02 5.6871593e-01]\n",
      " [2.3300000e+02 1.7500000e+02 3.4998143e-01]\n",
      " [2.3300000e+02 1.5500000e+02 3.1798881e-01]\n",
      " [2.2300000e+02 1.4700000e+02 3.3954647e-01]\n",
      " [2.2100000e+02 1.3500000e+02 3.7529767e-01]\n",
      " [1.8100000e+02 1.4700000e+02 3.6134517e-01]\n",
      " [1.5700000e+02 1.4700000e+02 4.1312248e-01]\n",
      " [1.4100000e+02 1.4500000e+02 3.7410533e-01]\n",
      " [1.2900000e+02 1.4500000e+02 4.0373942e-01]\n",
      " [1.8100000e+02 1.4900000e+02 4.0243661e-01]\n",
      " [1.5300000e+02 1.4700000e+02 4.3941677e-01]\n",
      " [1.4500000e+02 1.4700000e+02 3.9660519e-01]\n",
      " [1.2900000e+02 1.4700000e+02 3.6908132e-01]\n",
      " [1.8100000e+02 1.5700000e+02 4.5997694e-01]\n",
      " [1.5700000e+02 1.5100000e+02 4.0571016e-01]\n",
      " [1.4500000e+02 1.4700000e+02 3.6549553e-01]\n",
      " [1.4100000e+02 1.4700000e+02 3.0742222e-01]\n",
      " [1.7100000e+02 1.5900000e+02 4.8439163e-01]\n",
      " [1.6100000e+02 1.5500000e+02 3.7349096e-01]\n",
      " [1.5700000e+02 1.5500000e+02 3.3390918e-01]\n",
      " [1.4300000e+02 1.4900000e+02 2.7426872e-01]\n",
      " [8.1000000e+01 2.7500000e+02 6.0852647e-01]\n",
      " [1.1700000e+02 2.5700000e+02 4.1061756e-01]\n",
      " [1.2300000e+02 2.5500000e+02 4.5399913e-01]\n",
      " [1.3700000e+02 2.4500000e+02 4.2111921e-01]\n",
      " [1.5100000e+02 2.3100000e+02 4.3123174e-01]\n",
      " [1.2900000e+02 2.2300000e+02 5.2832770e-01]\n",
      " [1.3100000e+02 2.1700000e+02 5.0333059e-01]\n",
      " [1.4100000e+02 2.0300000e+02 5.0313616e-01]\n",
      " [1.4100000e+02 1.9300000e+02 6.6357040e-01]\n",
      " [1.2700000e+02 2.2300000e+02 5.3892684e-01]\n",
      " [1.2900000e+02 2.1300000e+02 5.7757628e-01]\n",
      " [1.3700000e+02 1.9700000e+02 5.7174706e-01]\n",
      " [1.3700000e+02 1.9300000e+02 7.1131361e-01]\n",
      " [1.1300000e+02 2.2500000e+02 5.3643769e-01]\n",
      " [1.2300000e+02 2.1300000e+02 5.1574516e-01]\n",
      " [1.2300000e+02 2.0500000e+02 4.7945017e-01]\n",
      " [1.3500000e+02 1.9500000e+02 5.7799089e-01]\n",
      " [1.0700000e+02 2.2300000e+02 6.6356564e-01]\n",
      " [1.1100000e+02 2.1900000e+02 6.3083339e-01]\n",
      " [1.1100000e+02 2.1500000e+02 5.4750252e-01]\n",
      " [1.1100000e+02 2.1100000e+02 4.6393597e-01]]\n",
      "(512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # load pretrained wholebody estimation model\n",
    "    config = 'wholebody_w48_384x288.yaml'\n",
    "    cfg.merge_from_file(config)\n",
    "\n",
    "    model = get_pose_net(cfg, is_train=False)\n",
    "    checkpoint = torch.load('./hrnet_w48_coco_wholebody_384x288-6e061c6a_20200922.pth')\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        # how and why remove modules?\n",
    "        if 'backbone.' in k:\n",
    "            name = k[9:] # remove module.\n",
    "        if 'keypoint_head.' in k:\n",
    "            name = k[14:] # remove module.\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # load and convert image\n",
    "    sample_img = os.path.join(image_path, 'frame_35.png')\n",
    "    img = cv2.imread(sample_img)\n",
    "    height, width = img.shape[:2]\n",
    "    img = cv2.flip(img, flipCode=1) # why flip from the beginning?\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # apply multi-scales: why?\n",
    "    multi_scales = [512, 640] # assuming input image is already 512x512\n",
    "    out = []\n",
    "    for scale in multi_scales:\n",
    "        print(scale)\n",
    "        if scale != 512: # if 640\n",
    "            img_temp = cv2.resize(img, (scale, scale)) # upscale 512x512 -> 640x640\n",
    "        else:\n",
    "            img_temp = img # original 512x512\n",
    "\n",
    "        img_temp = stack_flip(img_temp) # [2 images, height, width, channel]\n",
    "        print('stack flipped:', img_temp.shape)\n",
    "        img_temp = norm_numpy_totensor(img_temp, mean, std) # [2 images, channel, height, width]\n",
    "        print('norm numpy tensored:', img_temp.shape)\n",
    "\n",
    "        hms = model(img_temp)\n",
    "        print('hms:', hms.shape)\n",
    "\n",
    "        if scale != 512: # if 640\n",
    "            out.append(f.interpolate(hms, (width // 4, height // 4), mode='bilinear')) # 160x160 -> 128x128\n",
    "        else:\n",
    "            out.append(hms)\n",
    "\n",
    "        for element in out:\n",
    "            print('output element:', element.shape)\n",
    "\n",
    "        print()\n",
    "        \n",
    "    out = merge_hm(out)\n",
    "    print('merged:', out.shape)\n",
    "\n",
    "    result = out.reshape((133, -1))\n",
    "    result = torch.argmax(result, dim=1)\n",
    "    result = result.numpy().squeeze()\n",
    "    print('result:', result.shape)\n",
    "\n",
    "    y = result // (width // 4)\n",
    "    x = result % (width // 4) # what if width != height?\n",
    "    pred = np.zeros((133, 3), dtype=np.float32) # third axis?\n",
    "    pred[:, 0] = x\n",
    "    pred[:, 1] = y\n",
    "    pred = pose_process(pred, out)\n",
    "    pred[:,:2] *= 4.0 \n",
    "    print(pred.shape)\n",
    "    print('pred:', pred)\n",
    "\n",
    "img = plot_pose(img, pred)\n",
    "print(img.shape)\n",
    "cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('sample_output.png', img)\n",
    "\n",
    "# final numpy array: [frame, keypoint, x, y, ?]\n",
    "# what if not flip and not multi scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1a104cc5a30ffb61dcba35c048d59e2a2543d3b82917e7c4c5fa986c3cfbc1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
