{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "from natsort import natsorted\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from pose_hrnet import get_pose_net\n",
    "from config import cfg\n",
    "from config import update_config\n",
    "from utils import pose_process, plot_pose\n",
    "\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mirror = np.concatenate([\n",
    "                [1,3,2,5,4,7,6,9,8,11,10,13,12,15,14,17,16],\n",
    "                [21,22,23,18,19,20],\n",
    "                np.arange(40,23,-1), np.arange(50,40,-1),\n",
    "                np.arange(51,55), np.arange(59,54,-1),\n",
    "                [69,68,67,66,71,70], [63,62,61,60,65,64],\n",
    "                np.arange(78,71,-1), np.arange(83,78,-1),\n",
    "                [88,87,86,85,84,91,90,89],\n",
    "                np.arange(113,134), np.arange(92,113)\n",
    "                ]) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_flip(img):\n",
    "    img_flip = cv2.flip(img, 1)\n",
    "    return np.stack([img, img_flip], axis=0) # [img, height, width, channel]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_numpy_totensor(img, mean, std):\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    for i in range(3):\n",
    "        # img: [batch, height, width, channel]?\n",
    "        img[:, :, :, i] = (img[:, :, :, i] - mean[i]) / std[i] \n",
    "    return torch.from_numpy(img).permute(0, 3, 1, 2) # [img, channel, height, width]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hm(hms_list):\n",
    "    assert isinstance(hms_list, list) # hms_list?\n",
    "    for hms in hms_list:\n",
    "        hms[1,:,:,:] = torch.flip(hms[1,index_mirror,:,:], [2]) # hms[1] double flipped, so original? why flip on [2] height?\n",
    "    \n",
    "    hm = torch.cat(hms_list, dim=0)\n",
    "    hm = torch.mean(hms, dim=0)\n",
    "    return hm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = 'KETI_SL_0000002337.avi'\n",
    "video_path = os.path.join('./video', video_file)\n",
    "image_path = os.path.join('./image', video_file)\n",
    "\n",
    "mean = (0.485, 0.456, 0.406) # mean of normalized RGB? where did it come from?\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height = 1280, 720\n",
      "total 127 frames\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f'width, height = {w}, {h}')\n",
    "\n",
    "t = 0\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    t += 1\n",
    "\n",
    "    cv2.imwrite(os.path.join(image_path, f'frame_{t}.png'), image)\n",
    "\n",
    "cap.release()\n",
    "print(f'total {t} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width, height = 1280, 720\n",
      "total 127 frames\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(f'width, height = {w}, {h}')\n",
    "\n",
    "margin = int((w - h) / 2)\n",
    "\n",
    "t = 0\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    t += 1\n",
    "\n",
    "    image = image[:, margin : margin + h]\n",
    "    image = cv2.resize(image, (512, 512))\n",
    "    # print(image.shape)\n",
    "    cv2.imwrite(os.path.join(image_path, f'frame_{t}.png'), image)\n",
    "\n",
    "cap.release()\n",
    "print(f'total {t} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "stack flipped: (2, 512, 512, 3)\n",
      "norm numpy tensored: torch.Size([2, 3, 512, 512])\n",
      "hms: torch.Size([2, 133, 128, 128])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "\n",
      "640\n",
      "stack flipped: (2, 640, 640, 3)\n",
      "norm numpy tensored: torch.Size([2, 3, 640, 640])\n",
      "hms: torch.Size([2, 133, 160, 160])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "output element: torch.Size([2, 133, 128, 128])\n",
      "\n",
      "merged: torch.Size([133, 128, 128])\n",
      "result: (133,)\n",
      "pred: (133, 3)\n",
      "(512, 512, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # load pretrained wholebody estimation model\n",
    "    config = 'wholebody_w48_384x288.yaml'\n",
    "    cfg.merge_from_file(config)\n",
    "\n",
    "    model = get_pose_net(cfg, is_train=False)\n",
    "    checkpoint = torch.load('./hrnet_w48_coco_wholebody_384x288-6e061c6a_20200922.pth')\n",
    "    state_dict = checkpoint['state_dict']\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        # how and why remove modules?\n",
    "        if 'backbone.' in k:\n",
    "            name = k[9:] # remove module.\n",
    "        if 'keypoint_head.' in k:\n",
    "            name = k[14:] # remove module.\n",
    "        new_state_dict[name] = v\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # load and convert image\n",
    "    sample_img = os.path.join(image_path, 'frame_35.png')\n",
    "    img = cv2.imread(sample_img)\n",
    "    height, width = img.shape[:2]\n",
    "    img = cv2.flip(img, flipCode=1) # why flip from the beginning?\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # apply multi-scales: why?\n",
    "    multi_scales = [512, 640] # assuming input image is already 512x512\n",
    "    out = []\n",
    "    for scale in multi_scales:\n",
    "        print(scale)\n",
    "        if scale != 512: # if 640\n",
    "            img_temp = cv2.resize(img, (scale, scale)) # upscale 512x512 -> 640x640\n",
    "        else:\n",
    "            img_temp = img # original 512x512\n",
    "\n",
    "        img_temp = stack_flip(img_temp) # [2 images, height, width, channel]\n",
    "        print('stack flipped:', img_temp.shape)\n",
    "        img_temp = norm_numpy_totensor(img_temp, mean, std) # [2 images, channel, height, width]\n",
    "        print('norm numpy tensored:', img_temp.shape)\n",
    "\n",
    "        hms = model(img_temp)\n",
    "        print('hms:', hms.shape)\n",
    "\n",
    "        if scale != 512: # if 640\n",
    "            out.append(f.interpolate(hms, (width // 4, height // 4), mode='bilinear')) # 160x160 -> 128x128\n",
    "        else:\n",
    "            out.append(hms)\n",
    "\n",
    "        for element in out:\n",
    "            print('output element:', element.shape)\n",
    "\n",
    "        print()\n",
    "        \n",
    "    out = merge_hm(out)\n",
    "    print('merged:', out.shape)\n",
    "\n",
    "    result = out.reshape((133, -1))\n",
    "    result = torch.argmax(result, dim=1)\n",
    "    result = result.numpy().squeeze()\n",
    "    print('result:', result.shape)\n",
    "\n",
    "    y = result // (width // 4)\n",
    "    x = result % (width // 4) # what if width != height?\n",
    "    pred = np.zeros((133, 3), dtype=np.float32) # third axis?\n",
    "    pred[:, 0] = x\n",
    "    pred[:, 1] = y\n",
    "    pred = pose_process(pred, out)\n",
    "    pred[:,:2] *= 4.0 \n",
    "    print('pred:', pred.shape)\n",
    "\n",
    "img = plot_pose(img, pred)\n",
    "print(img.shape)\n",
    "cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "cv2.imwrite('sample_output.png', img)\n",
    "\n",
    "# final numpy array: [frame, keypoint, x, y, ?]\n",
    "# what if not flip and not multi scale?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d1a104cc5a30ffb61dcba35c048d59e2a2543d3b82917e7c4c5fa986c3cfbc1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
